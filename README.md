# ğŸ—‚ï¸ Local AWS-Style Mini Data Lake

A lightweight, AWS-style data lake implemented locally using Python.  
The project simulates the raw â†’ processed â†’ curated architecture found in cloud data lakes, complete with data cleaning, transformation, normalization, and Parquet-based partitioning.

---

## â­ Features

- **Data Lake Zones**
  - `raw/` â€“ unprocessed incoming data  
  - `processed/` â€“ cleaned & normalized data  
  - `curated/` â€“ optimized, partitioned parquet files  

- **Python ETL Pipeline**
  - Cleaning missing & inconsistent values  
  - Category normalization  
  - Price column standardization  
  - Synthetic date assignment for partitioning  
  - Year-based Parquet output  

- **Tech Stack**
  - Python  
  - Pandas  
  - PyArrow  

---

## ğŸ“‚ Folder Structure
```

local-data-lake/
â”‚â”€â”€ raw/
â”‚â”€â”€ processed/
â”‚â”€â”€ curated/
â”‚â”€â”€ scripts/
â”‚     â”œâ”€â”€ process.py
â”‚     â””â”€â”€ transform.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .gitignore
â”‚â”€â”€ README.md

````

---

## ğŸš€ How to Run the Project

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
````

### 2ï¸âƒ£ Add your dataset

Place your dataset here:

```
raw/sales.csv
```

### 3ï¸âƒ£ Process the data (cleaning + normalization)

```bash
python scripts/process.py
```

Output:

```
processed/sales_cleaned.csv
```

### 4ï¸âƒ£ Transform & partition the data (curated zone)

```bash
python scripts/transform.py
```

Output example:

```
curated/
   year=2022/sales.parquet
   year=2023/sales.parquet
```

---

## ğŸ›  Example Transformations

* Converted price fields to numeric
* Normalized category values
* Fixed missing rating fields
* Added synthetic partitioning dates
* Partitioned output by year using Parquet

---

## ğŸ“Š Example Use Cases

* Demonstrating data lake architecture without AWS
* Practicing ETL & data engineering fundamentals
* Portfolio project for Data Engineering interviews
* A template for future cloud migration (S3/Glue/Athena)

---

## ğŸ“Œ Future Enhancements

* Add metadata catalog
* Add schema validation
* Add real ingestion script
* Convert to Delta Lake format
* Add Airflow/Dagster orchestration


Just tell me!
```
